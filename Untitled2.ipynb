{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69c7d553",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import itertools\n",
    "import logging\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "from datasets import *\n",
    "# from dataset_usps import *\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "from trainer_cogan_mnist2usps import *\n",
    "# from net_config import *\n",
    "from optparse import OptionParser\n",
    "import random\n",
    "\n",
    "from stack_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7c56a742",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "45241bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"6\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5b0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torchvision \n",
    "\n",
    "# classify = torchvision.models.resnet18()\n",
    "# classify.fc = torch.nn.Linear(512, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "22c3067a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = .75\n",
    "BATCH_SIZE = 32\n",
    "LATENT_DIMS = 50\n",
    "MSE_WEIGHT = 0.01\n",
    "CLS_WEIGHT = 10.0\n",
    "\n",
    "TAN_DIR = \"./tan_imagery/\"\n",
    "MEX_DIR = \"./mex_imagery/\"\n",
    "\n",
    "TAN_TRAIN_FILES = [_ for _ in os.listdir(TAN_DIR) if _.endswith(\".png\")]\n",
    "MEX_TRAIN_FILES = [_ for _ in os.listdir(MEX_DIR) if _.endswith(\".png\")]\n",
    "\n",
    "TAN_TRAIN_INDICES = random.sample(range(len(TAN_TRAIN_FILES)), int(len(TAN_TRAIN_FILES) * SPLIT))\n",
    "MEX_TRAIN_INDICES = random.sample(range(len(MEX_TRAIN_FILES)), int(len(MEX_TRAIN_FILES) * SPLIT))\n",
    "TAN_VAL_INDICES = [_ for _ in range(len(TAN_TRAIN_FILES)) if _ not in TAN_TRAIN_INDICES]\n",
    "MEX_VAL_INDICES = [_ for _ in range(len(MEX_TRAIN_FILES)) if _ not in MEX_TRAIN_INDICES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec2f6bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NGF:  1024\n"
     ]
    }
   ],
   "source": [
    "trainer = Tan2MexCoGANTrainer(BATCH_SIZE, LATENT_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa6830a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of Tanzania training images:  11900\n",
      "Number of Mexico training images:  6519\n",
      "Number of Mexico validation images:  2174\n"
     ]
    }
   ],
   "source": [
    "train_dataset_a = TAN_DATASET(TAN_DIR, TAN_TRAIN_INDICES, \"./data/data_for_gan.csv\", BATCH_SIZE)\n",
    "\n",
    "train_dataset_b = MEX_DATASET(MEX_DIR, MEX_TRAIN_INDICES, \"./data/data_for_gan.csv\", BATCH_SIZE)\n",
    "test_dataset_b = MEX_DATASET(MEX_DIR, MEX_VAL_INDICES, \"./data/data_for_gan.csv\", BATCH_SIZE)\n",
    "\n",
    "print(\"Number of Tanzania training images: \", len(train_dataset_a))\n",
    "print(\"Number of Mexico training images: \", len(train_dataset_b))\n",
    "print(\"Number of Mexico validation images: \", len(test_dataset_b))\n",
    "\n",
    "\n",
    "train_loader_a = torch.utils.data.DataLoader(dataset = train_dataset_a, batch_size = BATCH_SIZE, shuffle = True)\n",
    "train_loader_b = torch.utils.data.DataLoader(dataset = train_dataset_b, batch_size = BATCH_SIZE, shuffle = True)\n",
    "test_loader_b = torch.utils.data.DataLoader(dataset = test_dataset_b, batch_size = BATCH_SIZE, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44f61939",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "matplotlib.use('Agg')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def imshow(inp, it, title = None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    plt.figure(figsize=(20,10))\n",
    "    plt.imshow(inp)\n",
    "    plt.savefig(f\"./outputs/it_{str(it)}.png\")\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "    plt.ioff()\n",
    "    plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d419ffae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use the to keep track of our training stastics (i.e. running training loss, running validation loss, etc...)\n",
    "class AverageMeter:\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n = 1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = round(self.sum / self.count, 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0de3ff5",
   "metadata": {},
   "source": [
    "#### INSTEAD OF USING A TEXT EMBEDDING LIKE BEFORE INT EH STACKGAN, USE THE CENSUS DATA, THEFORE YOU'RE LEARNING THE DA BOTH ON THE TABULAR DATA AND THE IMAGERY??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1a26e6db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 530, 530])\n",
      "Average Discriminator Accuracy:  0.508\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_loss_trackerr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-9ef81b114279>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Discriminator Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_discrim_acc_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Loss: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loss_trackerr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Average Class Accuracy: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_class_acc_tracker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_loss_trackerr' is not defined"
     ]
    }
   ],
   "source": [
    "train_discrim_acc_tracker, train_loss_tracker, train_class_acc_tracker = AverageMeter(), AverageMeter(), AverageMeter()\n",
    "\n",
    "for it, ((images_a, labels_a), (images_b, labels_b)) in enumerate(zip(train_loader_a, train_loader_b)):\n",
    "\n",
    "    if (images_a.shape[0] == BATCH_SIZE) and (images_b.shape[0] == BATCH_SIZE):\n",
    "\n",
    "        images_a, labels_a = images_a.cuda(), labels_a.squeeze().cuda()\n",
    "        images_b = images_b.cuda()\n",
    "        noise = Variable(torch.randn(BATCH_SIZE, LATENT_DIMS)).cuda()\n",
    "\n",
    "        ad_acc, mse_loss, cls_acc = trainer.dis_update(images_a, images_b, labels_a, noise)\n",
    "\n",
    "        noise = Variable(torch.randn(BATCH_SIZE, LATENT_DIMS)).cuda()\n",
    "\n",
    "        fake_images_a, fake_images_b = trainer.gen_update(noise)\n",
    "\n",
    "        train_discrim_acc_tracker.update(ad_acc.item())\n",
    "        train_loss_tracker.update(mse_loss.item())\n",
    "        train_class_acc_tracker.update(cls_acc.item())\n",
    "\n",
    "        if it % 10 == 0:\n",
    "\n",
    "            out = torchvision.utils.make_grid(torch.cat((images_a[0:16].cpu(), fake_images_a[0:16].cpu(), images_b[0:16].cpu(), fake_images_b[0:16].cpu())))\n",
    "            imshow(out, it, title = it) \n",
    "        \n",
    "        \n",
    "print(\"Average Discriminator Accuracy: \", train_discrim_acc_tracker.avg)\n",
    "print(\"Average Loss: \", train_loss_tracker.avg)\n",
    "print(\"Average Class Accuracy: \", train_class_acc_tracker.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d33a6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Discriminator Accuracy:  0.508\n",
      "Average Loss:  24.0039\n",
      "Average Class Accuracy:  0.8187\n"
     ]
    }
   ],
   "source": [
    "print(\"Average Discriminator Accuracy: \", train_discrim_acc_tracker.avg)\n",
    "print(\"Average Loss: \", train_loss_tracker.avg)\n",
    "print(\"Average Class Accuracy: \", train_class_acc_tracker.avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83abd6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(images_a[0].permute(1,2,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92250d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1082452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_images_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78584bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = torchvision.utils.make_grid(images_a.cpu())\n",
    "imshow(out[0:16], 0, title = 0)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5f7217",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
